{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T04:38:40.784384822Z",
     "start_time": "2025-11-01T04:38:40.776880597Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Hugging Face ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,      # Causal Language Model ë¡œë“œ\n",
    "    AutoTokenizer,              # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    BitsAndBytesConfig,         # ì–‘ìí™” ì„¤ì •\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "# TRL: Transformer Reinforcement Learning ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# PEFT: Parameter-Efficient Fine-Tuning\n",
    "from peft import (\n",
    "    LoraConfig,                          # LoRA ì„¤ì •\n",
    "    get_peft_model,                      # ëª¨ë¸ì— PEFT ì ìš©\n",
    "    prepare_model_for_kbit_training,     # ì–‘ìí™” ëª¨ë¸ í•™ìŠµ ì¤€ë¹„\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# GPU ì •ë³´ í™•ì¸\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ì„± í™•ë³´)\n",
    "SEED = 1210 # ë‚´ ìƒì¼ ã…ã…\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d37080e571bb98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T04:38:40.966745021Z",
     "start_time": "2025-11-01T04:38:40.784946739Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "TRAIN_DATA_PATH = \"./training_dataset\"  # ì €ì¥ëœ í•™ìŠµ ë°ì´í„°\n",
    "TEST_DATA_PATH = \"./testing_dataset\"    # ì €ì¥ëœ í‰ê°€ ë°ì´í„°\n",
    "\n",
    "print(\"ğŸ“‚ ì €ì¥ëœ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "# ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "# ì´ë¯¸ Dataset.save_to_disk()ë¡œ ì €ì¥ëœ í˜•ì‹ì´ë¯€ë¡œ ë¹ ë¥´ê²Œ ë¡œë“œ ê°€ëŠ¥\n",
    "try:\n",
    "    train_dataset = load_from_disk(TRAIN_DATA_PATH)\n",
    "    eval_dataset = load_from_disk(TEST_DATA_PATH)\n",
    "    \n",
    "    print(f\"âœ… í•™ìŠµ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ\")\n",
    "    print(f\"âœ… í‰ê°€ ë°ì´í„°: {len(eval_dataset)} ìƒ˜í”Œ\")\n",
    "    \n",
    "    # ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "    print(\"\\nğŸ“ ë°ì´í„° ìƒ˜í”Œ (ì²« ë²ˆì§¸):\")\n",
    "    print(f\"Query (ì²˜ìŒ 300ì):\\n{train_dataset[0]['query'][:300]}...\")\n",
    "    print(f\"\\nAnswer: {train_dataset[0]['answer']}\")\n",
    "    \n",
    "    # ë°ì´í„° í˜•ì‹ í™•ì¸\n",
    "    print(\"\\nğŸ” ë°ì´í„° ì»¬ëŸ¼:\", train_dataset.column_names)\n",
    "    print(\"âœ… ë°ì´í„° í˜•ì‹ í™•ì¸ ì™„ë£Œ\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ êµ¬ì¡° ì²´í¬\n",
    "    assert 'query' in train_dataset.column_names, \"âŒ 'query' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\"\n",
    "    assert 'answer' in train_dataset.column_names, \"âŒ 'answer' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\"\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"   ë‹¤ìŒ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    print(f\"   - {TRAIN_DATA_PATH}\")\n",
    "    print(f\"   - {TEST_DATA_PATH}\")\n",
    "    print(\"\\nğŸ’¡ ë°ì´í„°ì…‹ì„ ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    raise\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í˜•ì‹ í™•ì¸\n",
    "print(\"\\nğŸ“‹ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë¶„ì„:\")\n",
    "sample_query = train_dataset[0]['query']\n",
    "has_think_tag = '<think>' in sample_query\n",
    "has_answer_tag = '<answer>' in sample_query\n",
    "print(f\"  - <think> íƒœê·¸ í¬í•¨: {'âœ…' if has_think_tag else 'âŒ'}\")\n",
    "print(f\"  - <answer> íƒœê·¸ í¬í•¨: {'âœ…' if has_answer_tag else 'âŒ'}\")\n",
    "\n",
    "if has_think_tag and has_answer_tag:\n",
    "    print(\"âœ… ì¶”ë¡  + ì •ë‹µ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  ì¼ë°˜ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5115a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„¤ì •\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"  # ì‚¬ìš©í•  ëª¨ë¸ëª…\n",
    "\n",
    "# ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ 4bit ì–‘ìí™”)\n",
    "# BitsAndBytesë¥¼ ì‚¬ìš©í•œ NF4(Normal Float 4) ì–‘ìí™”\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # 4bit ì–‘ìí™” í™œì„±í™”\n",
    "    bnb_4bit_quant_type=\"nf4\",            # NF4 ì–‘ìí™” íƒ€ì… (ì¼ë°˜ì ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # ê³„ì‚°ì— ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…\n",
    "    bnb_4bit_use_double_quant=True,       # ì´ì¤‘ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆê°\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "# device_map=\"auto\": ìë™ìœ¼ë¡œ ì—¬ëŸ¬ GPUì— ëª¨ë¸ ë¶„ì‚° ë°°ì¹˜\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,      # ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "    device_map=\"auto\",                    # GPUì— ìë™ ë°°ì¹˜\n",
    "    trust_remote_code=True,               # ì»¤ìŠ¤í…€ ì½”ë“œ ì‹¤í–‰ í—ˆìš© (Qwen ëª¨ë¸)\n",
    "    torch_dtype=torch.bfloat16,           # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ bfloat16 ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",  # GRPOëŠ” left padding ê¶Œì¥ (ìƒì„± ì‹œ ì¼ê´€ì„±)\n",
    ")\n",
    "\n",
    "# íŒ¨ë”© í† í° ì„¤ì •\n",
    "# Qwen ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ eos_token ì‚¬ìš©\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\"âš™ï¸  íŒ¨ë”© í† í° ì„¤ì •: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "print(f\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"ğŸ’¾ ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì •ë³´\n",
    "print(f\"\\nğŸ“ í† í¬ë‚˜ì´ì € ì •ë³´:\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Padding side: {tokenizer.padding_side}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA(Low-Rank Adaptation) ì„¤ì •\n",
    "# LoRAëŠ” ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ëŒ€ì‹  ì‘ì€ í–‰ë ¬ë§Œ í•™ìŠµí•˜ì—¬ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì„ ì ˆì•½\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    r=32,                      # Low-rank í–‰ë ¬ì˜ rank (8, 16, 32ê°€ ì¼ë°˜ì )\n",
    "                               # ì‘ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ê³  ë¹ ë¦„, í´ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€\n",
    "    \n",
    "    lora_alpha=64,             # LoRA scaling factor (ì¼ë°˜ì ìœ¼ë¡œ rì˜ 2ë°°)\n",
    "                               # í•™ìŠµë¥ ê³¼ ê´€ë ¨ëœ ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°\n",
    "    \n",
    "    lora_dropout=0.05,         # Dropout ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)\n",
    "    \n",
    "    bias=\"none\",               # Bias í•™ìŠµ ì—¬ë¶€ (\"none\", \"all\", \"lora_only\")\n",
    "    \n",
    "    task_type=\"CAUSAL_LM\",     # íƒœìŠ¤í¬ íƒ€ì… (Causal Language Modeling)\n",
    "    \n",
    "    # LoRAë¥¼ ì ìš©í•  íƒ€ê²Ÿ ëª¨ë“ˆë“¤\n",
    "    # Qwen ëª¨ë¸ì˜ Attentionê³¼ FFN ë ˆì´ì–´ì˜ ì£¼ìš” íˆ¬ì˜ í–‰ë ¬ë“¤\n",
    "    target_modules=[\n",
    "        \"q_proj\",              # Query projection\n",
    "        \"k_proj\",              # Key projection  \n",
    "        \"v_proj\",              # Value projection\n",
    "        \"o_proj\",              # Output projection\n",
    "        \"gate_proj\",           # FFN gate projection\n",
    "        \"up_proj\",             # FFN up projection\n",
    "        \"down_proj\",           # FFN down projection\n",
    "    ],\n",
    "    \n",
    "    # ì¶”ê°€ ì„¤ì •\n",
    "    inference_mode=False,      # í•™ìŠµ ëª¨ë“œ\n",
    ")\n",
    "\n",
    "print(\"ğŸ”§ LoRA ì„¤ì •:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {len(lora_config.target_modules)} layers\")\n",
    "\n",
    "# ì–‘ìí™”ëœ ëª¨ë¸ì„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì¤€ë¹„\n",
    "# gradient checkpointing ë“±ì„ ìë™ìœ¼ë¡œ ì„¤ì •\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"âœ… ëª¨ë¸ì„ ì–‘ìí™” í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ\")\n",
    "\n",
    "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥\n",
    "model.print_trainable_parameters()\n",
    "# ì¶œë ¥ ì˜ˆì‹œ:\n",
    "# trainable params: 20M || all params: 8B || trainable%: 0.25%\n",
    "# â†’ ì „ì²´ ëª¨ë¸ì˜ 0.25%ë§Œ í•™ìŠµí•˜ë¯€ë¡œ ë§¤ìš° íš¨ìœ¨ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd77288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer_from_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì‘ë‹µì—ì„œ ë‹µë³€ ë²ˆí˜¸ ì¶”ì¶œ\n",
    "    \n",
    "    ì‚¬ìš©ìê°€ ì„¤ì •í•œ í˜•ì‹: <think>ì¶”ë¡ </think> <answer>ìˆ«ì</answer>\n",
    "    \n",
    "    Args:\n",
    "        response: ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        ì¶”ì¶œëœ ë‹µë³€ ë²ˆí˜¸ (1-5) ë˜ëŠ” ë¹ˆ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    # íŒ¨í„´ 1: <answer> íƒœê·¸ ì•ˆì˜ ìˆ«ì (ìµœìš°ì„ )\n",
    "    # ì˜ˆ: <answer>2</answer> ë˜ëŠ” <answer> 2 </answer>\n",
    "    pattern_answer_tag = r'<answer>\\s*([1-5])\\s*</answer>'\n",
    "    match = re.search(pattern_answer_tag, response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def compute_single_reward(query: str, response: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ ë³´ìƒ ê³„ì‚°\n",
    "    \n",
    "    GRPOì˜ í•µì‹¬: ì‘ë‹µì˜ í’ˆì§ˆì„ ì •ëŸ‰í™”í•˜ëŠ” ë³´ìƒ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        query: ì…ë ¥ ì§ˆë¬¸\n",
    "        response: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ\n",
    "        ground_truth: ì •ë‹µ\n",
    "    \n",
    "    Returns:\n",
    "        ë³´ìƒ ì ìˆ˜ (float)\n",
    "        - 1.0: ì •ë‹µ\n",
    "        - -0.5: ì˜¤ë‹µ\n",
    "        - -1.0: í˜•ì‹ ì˜¤ë¥˜ (ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨)\n",
    "    \n",
    "    Note:\n",
    "        ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ëŠ” GRPO ì„±ëŠ¥ì— ê²°ì •ì  ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "        - ë„ˆë¬´ sparseí•˜ë©´ í•™ìŠµì´ ì–´ë ¤ì›€\n",
    "        - ë„ˆë¬´ denseí•˜ë©´ overfitting ìœ„í—˜\n",
    "    \"\"\"\n",
    "    # ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ\n",
    "    predicted = extract_answer_from_response(response)\n",
    "    \n",
    "    # í˜•ì‹ ì˜¤ë¥˜: ë‹µë³€ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ\n",
    "    if not predicted:\n",
    "        return -1.0\n",
    "    \n",
    "    # ì •ë‹µ ì—¬ë¶€ í™•ì¸\n",
    "    if predicted == ground_truth:\n",
    "        # ì •ë‹µ: ë†’ì€ ì–‘ì˜ ë³´ìƒ\n",
    "        reward = 1.0\n",
    "        \n",
    "    else:\n",
    "        # ì˜¤ë‹µ: ìŒì˜ ë³´ìƒ (í•˜ì§€ë§Œ í˜•ì‹ ì˜¤ë¥˜ë³´ë‹¤ëŠ” ëœ ë‚˜ì¨)\n",
    "        reward = -0.5\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "def reward_function(samples: List[str], prompts: List[str], \n",
    "                   ground_truths: List[str], **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë‹¨ìœ„ ë³´ìƒ ê³„ì‚° í•¨ìˆ˜ (GRPO Trainerì—ì„œ í˜¸ì¶œ)\n",
    "    \n",
    "    Args:\n",
    "        samples: ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        prompts: ì…ë ¥ í”„ë¡¬í”„íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        ground_truths: ì •ë‹µ ë ˆì´ë¸”ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        **kwargs: ì¶”ê°€ ì¸ìë“¤ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
    "    \n",
    "    Returns:\n",
    "        ê° ìƒ˜í”Œì— ëŒ€í•œ ë³´ìƒ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "    \n",
    "    Note:\n",
    "        GRPOëŠ” ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ë³´ìƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        ê°™ì€ ì¿¼ë¦¬ì— ëŒ€í•œ ì—¬ëŸ¬ ì‘ë‹µì„ ë¹„êµí•˜ì—¬ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for sample, prompt, gt in zip(samples, prompts, ground_truths):\n",
    "        reward = compute_single_reward(prompt, sample, gt)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Reward Function í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ§ª Reward Function í…ŒìŠ¤íŠ¸ (ì‚¬ìš©ì í˜•ì‹ ê¸°ì¤€):\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"test query\", \"<think>ì´ ë¬¸ì œëŠ” ì§€ë¬¸ì˜ ì£¼ì œë¥¼ íŒŒì•…í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤...</think><answer>2</answer>\", \"2\", \"ì™„ë²½í•œ í˜•ì‹ - ì •ë‹µ\"),\n",
    "    (\"test query\", \"<think>ë¶„ì„ ê³¼ì •...</think> <answer>3</answer>\", \"3\", \"ê³µë°± í¬í•¨ - ì •ë‹µ\"),\n",
    "    (\"test query\", \"<think>ì¶”ë¡ ...</think><answer>2</answer>\", \"1\", \"ì™„ë²½í•œ í˜•ì‹ - ì˜¤ë‹µ\"),\n",
    "    (\"test query\", \"<answer>4</answer>\", \"4\", \"think ì—†ì´ answerë§Œ - ì •ë‹µ\"),\n",
    "    (\"test query\", \"<think>ê³ ë¯¼ ì¤‘...</think>ë‹µì€ 5ë²ˆì…ë‹ˆë‹¤\", \"5\", \"íƒœê·¸ ì—†ì´ í…ìŠ¤íŠ¸ - ì •ë‹µ\"),\n",
    "    (\"test query\", \"<think>ì¶”ë¡  ì¤‘...</think><answer>2\", \"2\", \"answer íƒœê·¸ ë¯¸ì™„ì„±\"),\n",
    "    (\"test query\", \"2ë²ˆì´ ì •ë‹µì…ë‹ˆë‹¤\", \"2\", \"íƒœê·¸ ì—†ëŠ” ì¼ë°˜ ë‹µë³€\"),\n",
    "    (\"test query\", \"ìŒ... ì–´ë ¤ìš´ ë¬¸ì œë„¤ìš”\", \"1\", \"í˜•ì‹ ì˜¤ë¥˜\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "for query, response, gt, desc in test_cases:\n",
    "    reward = compute_single_reward(query, response, gt)\n",
    "    extracted = extract_answer_from_response(response)\n",
    "    status = \"âœ…\" if extracted == gt else \"âŒ\"\n",
    "    print(f\"{status} {desc:30s} | ì¶”ì¶œ: {extracted:1s} | ì •ë‹µ: {gt} | ë³´ìƒ: {reward:+.2f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… Reward Function ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"\\nğŸ’¡ Tips:\")\n",
    "print(\"  - <answer> íƒœê·¸ ì•ˆì˜ ìˆ«ìê°€ ìµœìš°ì„ ìœ¼ë¡œ ì¶”ì¶œë©ë‹ˆë‹¤\")\n",
    "print(\"  - íƒœê·¸ê°€ ì—†ì–´ë„ ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ìˆ«ì ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤\")\n",
    "print(\"  - ì¶”ë¡  ê³¼ì •(<think>)ì€ ë³´ìƒ ê³„ì‚°ì— ì§ì ‘ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "OUTPUT_DIR = \"./grpo_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# GRPO í•™ìŠµ ì„¤ì •\n",
    "# GRPOConfigëŠ” TrainingArgumentsë¥¼ ìƒì†í•˜ë©° GRPO íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€\n",
    "training_args = GRPOConfig(\n",
    "    # ===== ê¸°ë³¸ ì„¤ì • =====\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    run_name=\"qwen_gpro_korean_ksat\",\n",
    "    \n",
    "    # ===== í•™ìŠµ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ===== \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,          # 1 â†’ 4 (4ë°° ì¦ê°€)\n",
    "    per_device_eval_batch_size=4,           # 1 â†’ 4\n",
    "    gradient_accumulation_steps=8,          # 16 â†’ 8 (ì‹¤ì§ˆì  ë°°ì¹˜: 4*8*3 = 96)\n",
    "    \n",
    "    # ===== ì˜µí‹°ë§ˆì´ì € ì„¤ì • =====\n",
    "    learning_rate=5e-6,                     # ë°°ì¹˜ í¬ê¸° ì¦ê°€ ì‹œ learning_rateë„ ì•½ê°„ ì¦ê°€ ê³ ë ¤ (7e-6 ì •ë„)\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # ===== GRPO íŠ¹í™” íŒŒë¼ë¯¸í„° =====\n",
    "    max_length=10000,\n",
    "    max_prompt_length=5000,\n",
    "    max_completion_length=5000,\n",
    "    \n",
    "    # ìƒì„± ì„¤ì •\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    \n",
    "    # GRPO í•µì‹¬ íŒŒë¼ë¯¸í„°\n",
    "    num_generations=8,                      # 4 â†’ 8 (ë” ë§ì€ ìƒ˜í”Œ ë¹„êµë¡œ ì„±ëŠ¥ í–¥ìƒ)\n",
    "    \n",
    "    # KL Divergence ì œì•½\n",
    "    kl_coef=0.05,\n",
    "    \n",
    "    # ===== ë¡œê¹… ë° ì²´í¬í¬ì¸íŠ¸ =====\n",
    "    logging_steps=5,                        # 10 â†’ 5 (ë” ìì£¼ ë¡œê¹…)\n",
    "    save_steps=50,                          # 100 â†’ 50\n",
    "    eval_steps=50,                          # 100 â†’ 50\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # ===== í‰ê°€ ì„¤ì • =====\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval/reward_mean\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # ===== í•˜ë“œì›¨ì–´ ìµœì í™” =====\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=8,               # 4 â†’ 8 (ë°ì´í„° ë¡œë”© ë³‘ë ¬í™” ì¦ê°€)\n",
    "    \n",
    "    # ===== ë¶„ì‚° í•™ìŠµ (Multi-GPU) =====\n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    "    # ===== ê¸°íƒ€ =====\n",
    "    seed=SEED,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\"âš™ï¸  GRPO í•™ìŠµ ì„¤ì •:\")\n",
    "print(f\"  ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * torch.cuda.device_count()}\")\n",
    "print(f\"  ì´ í•™ìŠµ ìŠ¤í… ìˆ˜: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * torch.cuda.device_count()) * training_args.num_train_epochs}\")\n",
    "print(f\"  ê° ì¿¼ë¦¬ë‹¹ ìƒì„± ìƒ˜í”Œ: {training_args.num_generations}\")\n",
    "print(f\"  KL Divergence ê³„ìˆ˜: {training_args.kl_coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¤ìŠ¤í…€ ì½œë°±: í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§\n",
    "import datetime\n",
    "\n",
    "\n",
    "class GRPOMonitorCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    GRPO í•™ìŠµì˜ ì£¼ìš” ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_reward = float('-inf')\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        ë¡œê¹… ë°œìƒ ì‹œ í˜¸ì¶œ\n",
    "        \"\"\"\n",
    "        if logs is None:\n",
    "            return\n",
    "        \n",
    "        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶œë ¥\n",
    "        if 'train/reward_mean' in logs:\n",
    "            reward = logs['train/reward_mean']\n",
    "            kl = logs.get('train/kl', 0)\n",
    "            loss = logs.get('loss', 0)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Step {state.global_step}:\")\n",
    "            print(f\"  â”œâ”€ Reward Mean: {reward:.4f}\")\n",
    "            print(f\"  â”œâ”€ KL Divergence: {kl:.6f}\")\n",
    "            print(f\"  â””â”€ Loss: {loss:.4f}\")\n",
    "            \n",
    "            # ìµœê³  ë³´ìƒ ì—…ë°ì´íŠ¸\n",
    "            if reward > self.best_reward:\n",
    "                self.best_reward = reward\n",
    "                print(f\"  â­ New best reward: {self.best_reward:.4f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"\n",
    "        í‰ê°€ ì™„ë£Œ ì‹œ í˜¸ì¶œ\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            return\n",
    "        \n",
    "        eval_reward = metrics.get('eval/reward_mean', 0)\n",
    "        print(f\"\\nğŸ“ˆ Evaluation at step {state.global_step}:\")\n",
    "        print(f\"  â””â”€ Eval Reward Mean: {eval_reward:.4f}\")\n",
    "\n",
    "\n",
    "# GRPO Trainer ì´ˆê¸°í™”\n",
    "print(\"\\nğŸš€ GRPO Trainer ì´ˆê¸°í™” ì¤‘...\")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,                            # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=training_args,                     # í•™ìŠµ ì„¤ì •\n",
    "    train_dataset=train_dataset,            # í•™ìŠµ ë°ì´í„°\n",
    "    eval_dataset=eval_dataset,              # í‰ê°€ ë°ì´í„°\n",
    "    tokenizer=tokenizer,                    # í† í¬ë‚˜ì´ì €\n",
    "    reward_fn=reward_function,              # ë³´ìƒ í•¨ìˆ˜\n",
    "    peft_config=lora_config,                # LoRA ì„¤ì •\n",
    "    callbacks=[GRPOMonitorCallback()],      # ì»¤ìŠ¤í…€ ì½œë°±\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ GRPO í•™ìŠµ ì‹œì‘\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # í•™ìŠµ ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nğŸ“Š ìµœì¢… í•™ìŠµ ê²°ê³¼:\")\n",
    "    print(f\"  â”œâ”€ Total training time: {train_result.metrics.get('train_runtime', 0):.2f} seconds\")\n",
    "    print(f\"  â”œâ”€ Training samples/second: {train_result.metrics.get('train_samples_per_second', 0):.2f}\")\n",
    "    print(f\"  â””â”€ Final loss: {train_result.metrics.get('train_loss', 0):.4f}\")\n",
    "    \n",
    "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "    final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸  í•™ìŠµì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"í˜„ì¬ê¹Œì§€ì˜ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# TensorBoard ë¡œê·¸ í™•ì¸ ì•ˆë‚´\n",
    "print(\"\\nğŸ“Š TensorBoardë¡œ í•™ìŠµ ê³¼ì • í™•ì¸:\")\n",
    "print(f\"  í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰: tensorboard --logdir {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def generate_answer(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    query: str, \n",
    "    max_new_tokens: int = 5000,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    í•™ìŠµëœ ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµëœ ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        query: ì…ë ¥ ì§ˆë¬¸\n",
    "        max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "        temperature: ìƒ˜í”Œë§ ì˜¨ë„\n",
    "        top_p: Nucleus sampling threshold\n",
    "        do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€ (Falseë©´ greedy decoding)\n",
    "    \n",
    "    Returns:\n",
    "        ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    model.eval()\n",
    "    \n",
    "    # ì…ë ¥ í† í¬ë‚˜ì´ì§•\n",
    "    inputs = tokenizer(\n",
    "        query,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=5000\n",
    "    )\n",
    "    \n",
    "    # GPUë¡œ ì´ë™\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=50,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def evaluate_model_on_dataset(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset: Dataset,\n",
    "    num_samples: int = None,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "    \n",
    "    Args:\n",
    "        model: í‰ê°€í•  ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        dataset: í‰ê°€ ë°ì´í„°ì…‹\n",
    "        num_samples: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)\n",
    "        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    format_errors = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nğŸ§ª ëª¨ë¸ í‰ê°€ ì‹œì‘ ({num_samples} ìƒ˜í”Œ)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        query = sample['query']\n",
    "        ground_truth = sample['answer']\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        response = generate_answer(model, tokenizer, query)\n",
    "        \n",
    "        # ë‹µë³€ ì¶”ì¶œ\n",
    "        predicted = extract_answer_from_response(response)\n",
    "        \n",
    "        # í‰ê°€\n",
    "        if not predicted:\n",
    "            format_errors += 1\n",
    "            is_correct = False\n",
    "        else:\n",
    "            is_correct = (predicted == ground_truth)\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'response': response,\n",
    "            'predicted': predicted,\n",
    "            'ground_truth': ground_truth,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "        \n",
    "        # ìƒì„¸ ì¶œë ¥\n",
    "        if verbose and i < 5:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
    "            print(f\"\\nìƒ˜í”Œ {i+1}:\")\n",
    "            print(f\"  Query (ì²˜ìŒ 100ì): {query[:100]}...\")\n",
    "            print(f\"  Response: {response}\")\n",
    "            print(f\"  Predicted: {predicted}\")\n",
    "            print(f\"  Ground Truth: {ground_truth}\")\n",
    "            print(f\"  Correct: {'âœ…' if is_correct else 'âŒ'}\")\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    format_error_rate = format_errors / total if total > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'format_errors': format_errors,\n",
    "        'format_error_rate': format_error_rate\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š í‰ê°€ ê²°ê³¼:\")\n",
    "    print(f\"  â”œâ”€ Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"  â”œâ”€ Correct: {correct}/{total}\")\n",
    "    print(f\"  â””â”€ Format Errors: {format_errors} ({format_error_rate*100:.2f}%)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return metrics, results\n",
    "\n",
    "\n",
    "# ===== í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë¡œ ë¹ ë¥¸ í™•ì¸ =====\n",
    "print(\"\\nğŸ¯ ë‹¨ì¼ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸:\")\n",
    "test_sample = eval_dataset[0]\n",
    "test_response = generate_answer(model, tokenizer, test_sample['query'])\n",
    "\n",
    "print(f\"Query (ì²˜ìŒ 300ì):\\n{test_sample['query'][:300]}...\\n\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Generated Response:\\n{test_response}\\n\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Ground Truth: {test_sample['answer']}\")\n",
    "\n",
    "# ë‹µë³€ ì¶”ì¶œ ë° í‰ê°€\n",
    "predicted = extract_answer_from_response(test_response)\n",
    "print(f\"Predicted: {predicted}\")\n",
    "\n",
    "# í˜•ì‹ ì²´í¬\n",
    "has_think = '<think>' in test_response.lower()\n",
    "has_answer = '<answer>' in test_response.lower()\n",
    "print(f\"\\ní˜•ì‹ í™•ì¸:\")\n",
    "print(f\"  - <think> íƒœê·¸ ì‚¬ìš©: {'âœ…' if has_think else 'âŒ'}\")\n",
    "print(f\"  - <answer> íƒœê·¸ ì‚¬ìš©: {'âœ…' if has_answer else 'âŒ'}\")\n",
    "\n",
    "# ì •ë‹µ ì—¬ë¶€\n",
    "is_correct = predicted == test_sample['answer']\n",
    "print(f\"\\nê²°ê³¼: {'âœ… Correct!' if is_correct else 'âŒ Wrong'}\")\n",
    "\n",
    "if not is_correct:\n",
    "    print(f\"ğŸ’¡ ëª¨ë¸ì´ {predicted}ë²ˆì„ ì„ íƒí–ˆì§€ë§Œ ì •ë‹µì€ {test_sample['answer']}ë²ˆì…ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ===== ì „ì²´ í‰ê°€ ë°ì´í„°ì…‹ í‰ê°€ =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "eval_metrics, eval_results = evaluate_model_on_dataset(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=eval_dataset,\n",
    "    num_samples=None,  # ì „ì²´ í‰ê°€\n",
    "    verbose=False  # ìƒ˜í”Œë³„ ì¶œë ¥ ë¹„í™œì„±í™”\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "results_save_path = os.path.join(OUTPUT_DIR, \"evaluation_results.json\")\n",
    "with open(results_save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'metrics': eval_metrics,\n",
    "        'results': eval_results[:10]  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {results_save_path}\")\n",
    "\n",
    "\n",
    "# ===== ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¬ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ëª¨ë“œ\")\n",
    "print(\"ì‚¬ìš©ë²•: ë…í•´ ë¬¸ì œë¥¼ ì…ë ¥í•˜ë©´ ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "print(\"       (í”„ë¡¬í”„íŠ¸ í˜•ì‹ì€ ìë™ìœ¼ë¡œ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)\")\n",
    "print(\"ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit' ì…ë ¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def interactive_test():\n",
    "    \"\"\"\n",
    "    ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "    \n",
    "    Note:\n",
    "        ì‚¬ìš©ìê°€ ì´ë¯¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ë§Œë“¤ì—ˆìœ¼ë¯€ë¡œ\n",
    "        ì¶”ê°€ í¬ë§·íŒ… ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        print(\"\\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” 'sample'ë¡œ ìƒ˜í”Œ ì‹¤í–‰):\")\n",
    "        user_input = input(\"> \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\n",
    "        if user_input.lower() == 'sample':\n",
    "            if len(eval_dataset) > 0:\n",
    "                import random\n",
    "                sample_idx = random.randint(0, len(eval_dataset) - 1)\n",
    "                user_input = eval_dataset[sample_idx]['query']\n",
    "                print(f\"\\n[ìƒ˜í”Œ #{sample_idx}]\")\n",
    "                print(f\"Ground Truth: {eval_dataset[sample_idx]['answer']}\")\n",
    "            else:\n",
    "                print(\"âŒ í‰ê°€ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        print(\"\\nğŸ¤” ìƒì„± ì¤‘...\")\n",
    "        response = generate_answer(model, tokenizer, user_input)\n",
    "        \n",
    "        print(f\"\\nğŸ“ ëª¨ë¸ ë‹µë³€:\\n{response}\")\n",
    "        \n",
    "        # ë‹µë³€ ì¶”ì¶œ\n",
    "        predicted = extract_answer_from_response(response)\n",
    "        if predicted:\n",
    "            print(f\"\\nâœ… ì¶”ì¶œëœ ë‹µë³€: {predicted}ë²ˆ\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸  ë‹µë³€ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰ (ì„ íƒì‚¬í•­)\n",
    "# ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰:\n",
    "# interactive_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
